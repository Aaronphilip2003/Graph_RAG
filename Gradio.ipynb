{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b89e7537-8509-4e6c-bb5f-e443579e666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gradio\n",
      "Version: 3.39.0\n",
      "Summary: Python library for easily interacting with trained machine learning models\n",
      "Home-page: https://github.com/gradio-app/gradio\n",
      "Author: \n",
      "Author-email: Abubakar Abid <team@gradio.app>, Ali Abid <team@gradio.app>, Ali Abdalla <team@gradio.app>, Dawood Khan <team@gradio.app>, Ahsen Khaliq <team@gradio.app>, Pete Allen <team@gradio.app>, Ömer Faruk Özdemir <team@gradio.app>\n",
      "License: \n",
      "Location: /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages\n",
      "Requires: aiofiles, aiohttp, altair, fastapi, ffmpy, gradio-client, httpx, huggingface-hub, jinja2, markdown-it-py, markupsafe, matplotlib, mdit-py-plugins, numpy, orjson, packaging, pandas, pillow, pydantic, pydub, python-multipart, pyyaml, requests, semantic-version, typing-extensions, uvicorn, websockets\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aba51e71-638c-4e77-ac20-937f3b633fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==3.39.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (3.39.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (23.2.1)\n",
      "Requirement already satisfied: aiohttp~=3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (3.12.12)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.6.0)\n",
      "Requirement already satisfied: gradio-client>=0.3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (1.3.0)\n",
      "Requirement already satisfied: httpx in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.33.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (3.1.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.0.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.2.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (3.9.4)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.3.3)\n",
      "Requirement already satisfied: numpy~=1.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (3.10.18)\n",
      "Requirement already satisfied: packaging in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (2.3.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (10.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (2.11.5)\n",
      "Requirement already satisfied: pydub in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (6.0.2)\n",
      "Requirement already satisfied: requests~=2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (2.32.3)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (4.14.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (0.35.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio==3.39.0) (11.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from aiohttp~=3.0->gradio==3.39.0) (1.20.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (1.44.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from matplotlib~=3.0->gradio==3.39.0) (6.5.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from requests~=2.0->gradio==3.39.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from requests~=2.0->gradio==3.39.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from requests~=2.0->gradio==3.39.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from requests~=2.0->gradio==3.39.0) (2025.4.26)\n",
      "Requirement already satisfied: fsspec in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from gradio-client>=0.3.0->gradio==3.39.0) (2025.5.1)\n",
      "Requirement already satisfied: anyio in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from httpx->gradio==3.39.0) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from httpx->gradio==3.39.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from httpcore==1.*->httpx->gradio==3.39.0) (0.16.0)\n",
      "Requirement already satisfied: filelock in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (3.18.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (1.1.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib~=3.0->gradio==3.39.0) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.25.1)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.0.3)\n",
      "Requirement already satisfied: uc-micro-py in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (1.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.39.0) (1.17.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio==3.39.0) (8.1.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from anyio->httpx->gradio==3.39.0) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from anyio->httpx->gradio==3.39.0) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages (from fastapi->gradio==3.39.0) (0.47.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio==3.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcbb3753-19b4-49e6-91e6-8b544f0f57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "%run ./utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7b87bfd-ee5f-4e5b-9ba3-74532f222229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.39.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://6807a77678b19a45c4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6807a77678b19a45c4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio/blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio/blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio/utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/35/143126m50gv7zq_ldk50bbhr0000gn/T/ipykernel_67668/3122777835.py\", line 189, in store_embedded_chunks_to_chroma\n",
      "    chroma_collection.upsert(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/chromadb/api/models/Collection.py\", line 365, in upsert\n",
      "    upsert_request = self._validate_and_prepare_upsert_request(\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py\", line 95, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/chromadb/api/models/CollectionCommon.py\", line 415, in _validate_and_prepare_upsert_request\n",
      "    validate_insert_record_set(record_set=upsert_records)\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/chromadb/api/types.py\", line 312, in validate_insert_record_set\n",
      "    validate_ids(record_set[\"ids\"])\n",
      "  File \"/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/chromadb/api/types.py\", line 728, in validate_ids\n",
      "    raise errors.DuplicateIDError(message)\n",
      "chromadb.errors.DuplicateIDError: Expected IDs to be unique, found 210 duplicated IDs: metformin1.pdf_10_2, metformin1.pdf_23_3, metformin1.pdf_24_0, metformin1.pdf_21_6, metformin1.pdf_13_4, ..., metformin1.pdf_32_6, metformin1.pdf_29_4, metformin1.pdf_7_0, metformin1.pdf_8_4, metformin1.pdf_31_5 in upsert.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF\n",
    "from pymongo import MongoClient\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Ensure ./PDFs directory exists\n",
    "os.makedirs(\"./PDFs\", exist_ok=True)\n",
    "\n",
    "# MongoDB setup\n",
    "MONGO_URI = \"mongodb+srv://Aaron:1234@cluster0.erwea75.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "def get_mongo_client(uri):\n",
    "    return MongoClient(uri)\n",
    "\n",
    "def get_collection(client, db_name, collection_name):\n",
    "    return client[db_name][collection_name]\n",
    "\n",
    "def insert_pdf_pages_to_mongo(collection, pdf_name, pages):\n",
    "    for page in pages:\n",
    "        if not collection.find_one({\"pdf_name\": pdf_name, \"page_number\": page[\"page_number\"]}):\n",
    "            collection.insert_one({\n",
    "                \"pdf_name\": pdf_name,\n",
    "                \"page_number\": page[\"page_number\"],\n",
    "                \"text\": page[\"text\"]\n",
    "            })\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    pages = []\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc.load_page(page_number)\n",
    "        text = page.get_text().strip()\n",
    "        pages.append({\n",
    "            \"page_number\": page_number + 1,\n",
    "            \"text\": text\n",
    "        })\n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "# Upload Logic\n",
    "uploaded_pdfs = []\n",
    "\n",
    "def upload_docs(files):\n",
    "    saved_files = []\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file.name)\n",
    "        dest_path = os.path.join(\"./PDFs\", filename)\n",
    "\n",
    "        if os.path.abspath(file.name) != os.path.abspath(dest_path):\n",
    "            shutil.copy(file.name, dest_path)\n",
    "            saved_files.append(filename)\n",
    "        else:\n",
    "            saved_files.append(filename + \" (already exists)\")\n",
    "\n",
    "    global uploaded_pdfs\n",
    "    uploaded_pdfs = saved_files\n",
    "    return f\"Uploaded: {', '.join(saved_files)}\"\n",
    "\n",
    "# Mongo Upload Logic\n",
    "def upload_to_mongo():\n",
    "    if not uploaded_pdfs:\n",
    "        return \"❌ No PDFs uploaded yet.\"\n",
    "\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    collection = get_collection(client, \"pdf_rag_db\", \"pages\")\n",
    "\n",
    "    inserted = []\n",
    "    skipped = []\n",
    "\n",
    "    for pdf in uploaded_pdfs:\n",
    "        full_path = os.path.join(\"./PDFs\", pdf)\n",
    "\n",
    "        if collection.find_one({\"pdf_name\": pdf}):\n",
    "            skipped.append(pdf)\n",
    "            continue\n",
    "\n",
    "        pages = extract_text_from_pdf(full_path)\n",
    "        insert_pdf_pages_to_mongo(collection, pdf, pages)\n",
    "        inserted.append(pdf)\n",
    "\n",
    "    result = \"\"\n",
    "    if inserted:\n",
    "        result += f\"✅ Uploaded to MongoDB: {', '.join(inserted)}\\n\"\n",
    "    if skipped:\n",
    "        result += f\"⚠️ Skipped (already in DB): {', '.join(skipped)}\"\n",
    "    return result.strip()\n",
    "\n",
    "# Add this helper function to get list of uploaded PDFs\n",
    "def get_uploaded_pdfs():\n",
    "    return [f for f in os.listdir(\"./PDFs\") if f.endswith(\".pdf\")]\n",
    "\n",
    "# ------------------ CHUNKING LOGIC ------------------\n",
    "def chunking(\n",
    "    selected_pdf,\n",
    "    db_name: str = \"pdf_rag_db\",\n",
    "    collection_name: str = \"pages\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 100\n",
    ") -> str:\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    pages_collection = get_collection(client, db_name, collection_name)\n",
    "    documents = list(pages_collection.find({\"pdf_name\": selected_pdf}))\n",
    "    if not documents:\n",
    "        return f\"❌ No pages found in MongoDB for {selected_pdf}.\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc[\"text\"])\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                \"pdf_name\": doc[\"pdf_name\"],\n",
    "                \"page_number\": doc[\"page_number\"],\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "    if not chunked_docs:\n",
    "        return \"❌ No chunks created.\"\n",
    "\n",
    "    chunk_collection = get_collection(client, db_name, \"chunks\")\n",
    "    chunk_collection.insert_many(chunked_docs)\n",
    "    return f\"✅ Stored {len(chunked_docs)} chunks for {selected_pdf} in MongoDB.\"\n",
    "\n",
    "def display_chunks(selected_pdf):\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    collection = get_collection(client, \"pdf_rag_db\", \"chunks\")\n",
    "    chunks = list(collection.find({\"pdf_name\": selected_pdf}, {\"_id\": 0}))\n",
    "    return chunks if chunks else f\"⚠️ No chunks found in MongoDB for {selected_pdf}.\"\n",
    "\n",
    "# ------------------ EMBEDDING & CHROMADB ------------------\n",
    "def get_all_chroma_ids(collection):\n",
    "    all_ids = set()\n",
    "    offset = 0\n",
    "    batch_size = 100  # or 500, adjust as needed\n",
    "\n",
    "    while True:\n",
    "        result = collection.get(ids=None, limit=batch_size, offset=offset)\n",
    "        ids = result.get(\"ids\", [])\n",
    "        if not ids:\n",
    "            break\n",
    "        all_ids.update(ids)\n",
    "        offset += batch_size\n",
    "\n",
    "    return all_ids\n",
    "\n",
    "def store_embedded_chunks_to_chroma(selected_pdf):\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    chunks_collection = get_collection(client, \"pdf_rag_db\", \"chunks\")\n",
    "\n",
    "    chunks = list(chunks_collection.find({\"pdf_name\": selected_pdf}))\n",
    "    if not chunks:\n",
    "        return f\"❌ No chunks found for {selected_pdf}. Please chunk it first.\"\n",
    "    \n",
    "    chroma_client = chromadb.Client()\n",
    "    chroma_collection = chroma_client.get_or_create_collection(name=\"pdf_chunks\")\n",
    "\n",
    "    # ✅ Properly fetch ALL existing IDs (with pagination)\n",
    "    existing_ids = get_all_chroma_ids(chroma_collection)\n",
    "\n",
    "    new_ids, new_docs, new_metadata = [], [], []\n",
    "    for chunk in chunks:\n",
    "        chunk_id = f\"{chunk['pdf_name']}_{chunk['page_number']}_{chunk['chunk_index']}\"\n",
    "        if chunk_id in existing_ids:\n",
    "            continue\n",
    "        new_ids.append(chunk_id)\n",
    "        new_docs.append(chunk[\"chunk_text\"])\n",
    "        new_metadata.append({\n",
    "            \"pdf_name\": chunk[\"pdf_name\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"chunk_index\": chunk[\"chunk_index\"]\n",
    "        })\n",
    "\n",
    "    if not new_ids:\n",
    "        return f\"✅ All chunks of {selected_pdf} are already stored in ChromaDB.\"\n",
    "\n",
    "    chroma_collection.upsert(\n",
    "        ids=new_ids,\n",
    "        documents=new_docs,\n",
    "        metadatas=new_metadata\n",
    "    )\n",
    "\n",
    "    return f\"✅ Embedded and stored {len(new_ids)} new chunks for {selected_pdf} in ChromaDB.\"\n",
    "\n",
    "# Load the same model used for embedding\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def search_query(question):\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([question]).tolist()\n",
    "\n",
    "    # Connect to Chroma with the same settings used for embedding\n",
    "    client = chromadb.Client()  # <- must match your embedding setup\n",
    "    collection = client.get_collection(\"pdf_chunks\")\n",
    "\n",
    "    # Query top 5 matching chunks\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    if not results[\"documents\"]:\n",
    "        return \"⚠️ No matching chunks found.\"\n",
    "\n",
    "    chunks = results[\"documents\"][0]\n",
    "    metadata = results[\"metadatas\"][0]\n",
    "\n",
    "    formatted = []\n",
    "    for i in range(len(chunks)):\n",
    "        chunk = chunks[i]\n",
    "        meta = metadata[i]\n",
    "        formatted.append(\n",
    "            f\"🧩 Chunk {i+1} from page {meta.get('page_number', '?')}, index {meta.get('chunk_index', '?')}:\\n{chunk}\"\n",
    "        )\n",
    "    return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "# ------------------ UI ------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 PDF Upload + MongoDB + Knowledge Nuggets\")\n",
    "\n",
    "    with gr.Tab(\"Upload PDFs\"):\n",
    "        upload_input = gr.File(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "        upload_result = gr.Textbox(label=\"Upload Result\")\n",
    "        mongo_result = gr.Textbox(label=\"MongoDB Result\")\n",
    "\n",
    "        upload_btn = gr.Button(\"Upload to ./PDFs\")\n",
    "        mongo_btn = gr.Button(\"Upload Extracted Text to MongoDB\")\n",
    "\n",
    "        upload_btn.click(upload_docs, inputs=upload_input, outputs=upload_result)\n",
    "        mongo_btn.click(upload_to_mongo, outputs=mongo_result)\n",
    "\n",
    "    with gr.Tab(\"Knowledge Nuggets\"):\n",
    "        pdf_select_chunk = gr.Dropdown(label=\"Select PDF to Chunk\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        chunk_btn = gr.Button(\"🔪 Chunk PDF Pages\")\n",
    "        chunk_output = gr.Textbox(label=\"Chunking Result\")\n",
    "\n",
    "        pdf_select_display = gr.Dropdown(label=\"Select PDF to Display Chunks\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        display_btn = gr.Button(\"📚 Display Chunks\")\n",
    "        display_output = gr.JSON(label=\"Chunks from MongoDB\")\n",
    "\n",
    "        chunk_btn.click(chunking, inputs=pdf_select_chunk, outputs=chunk_output)\n",
    "        display_btn.click(display_chunks, inputs=pdf_select_display, outputs=display_output)\n",
    "        \n",
    "    with gr.Tab(\"Embed to Chroma\"):\n",
    "        # PDF selection for embedding\n",
    "        pdf_select_embed = gr.Dropdown(label=\"Select PDF to Embed\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        embed_btn = gr.Button(\"💾 Embed and Store in ChromaDB\")\n",
    "        embed_output = gr.Textbox(label=\"Embedding Result\")\n",
    "    \n",
    "        # Show all chunks from ChromaDB collection\n",
    "        view_btn = gr.Button(\"👁️ View All Chunks in ChromaDB\")\n",
    "        view_output = gr.Textbox(label=\"Chunks in ChromaDB\")\n",
    "    \n",
    "        # Embed selected PDF into ChromaDB\n",
    "        embed_btn.click(store_embedded_chunks_to_chroma, inputs=pdf_select_embed, outputs=embed_output)\n",
    "    \n",
    "        # Show all chunks (no PDF selection)\n",
    "        view_btn.click(fn=view_all_chunks, outputs=view_output)\n",
    "    \n",
    "        def view_all_chunks(collection_name: str = \"pdf_chunks\"):\n",
    "            client = chromadb.Client()\n",
    "            try:\n",
    "                collection = client.get_collection(name=collection_name)\n",
    "            except Exception as e:\n",
    "                return f\"❌ Error: {str(e)}\"\n",
    "        \n",
    "            results = collection.get(include=[\"documents\", \"embeddings\"], limit=500)\n",
    "        \n",
    "            chunks = []\n",
    "            for i, (doc, embedding) in enumerate(zip(results[\"documents\"], results[\"embeddings\"])):\n",
    "                chunks.append(f\"🧩 Chunk {i+1}:\\n{doc}\\n🔢 Embedding (first 5 values): {embedding[:5]}\\n{'-'*40}\")\n",
    "            return \"\\n\".join(chunks) if chunks else \"⚠️ No chunks found in collection.\"\n",
    "    \n",
    "    with gr.Tab(\"Query Knowledge Nuggets\"):\n",
    "        gr.Markdown(\"### Ask a question about the document\")\n",
    "    \n",
    "        query_input = gr.Textbox(label=\"Enter your question\", placeholder=\"e.g. What is the document about?\")\n",
    "        query_button = gr.Button(\"Search\")\n",
    "        query_output = gr.Textbox(label=\"Top 5 Matching Chunks\", lines=15)\n",
    "    \n",
    "        query_button.click(fn=search_query, inputs=query_input, outputs=query_output)\n",
    "\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3726272d-e966-4280-b05e-d1126f2ebff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted\n"
     ]
    }
   ],
   "source": [
    "def reset_chroma_collection(name=\"pdf_chunks\"):\n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        client.delete_collection(name)\n",
    "        print(\"Deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "    client.create_collection(name)\n",
    "\n",
    "reset_chroma_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda68f12-9c26-418d-9ee1-8f8c0a3c8970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

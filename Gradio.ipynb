{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbb3753-19b4-49e6-91e6-8b544f0f57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "%run ./utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b87bfd-ee5f-4e5b-9ba3-74532f222229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio/routes.py:26: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/Users/aaron/Documents/code/Deep-Learning-AI/deeplearningai/lib/python3.9/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.39.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://6e3ec10643b681eb21.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6e3ec10643b681eb21.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF\n",
    "from pymongo import MongoClient\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Ensure ./PDFs directory exists\n",
    "os.makedirs(\"./PDFs\", exist_ok=True)\n",
    "\n",
    "# MongoDB setup\n",
    "MONGO_URI = \"mongodb+srv://Aaron:1234@cluster0.erwea75.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "def get_mongo_client(uri):\n",
    "    return MongoClient(uri)\n",
    "\n",
    "def get_collection(client, db_name, collection_name):\n",
    "    return client[db_name][collection_name]\n",
    "\n",
    "def insert_pdf_pages_to_mongo(collection, pdf_name, pages):\n",
    "    for page in pages:\n",
    "        if not collection.find_one({\"pdf_name\": pdf_name, \"page_number\": page[\"page_number\"]}):\n",
    "            collection.insert_one({\n",
    "                \"pdf_name\": pdf_name,\n",
    "                \"page_number\": page[\"page_number\"],\n",
    "                \"text\": page[\"text\"]\n",
    "            })\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    pages = []\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc.load_page(page_number)\n",
    "        text = page.get_text().strip()\n",
    "        pages.append({\n",
    "            \"page_number\": page_number + 1,\n",
    "            \"text\": text\n",
    "        })\n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "# Upload Logic\n",
    "uploaded_pdfs = []\n",
    "\n",
    "def upload_docs(files):\n",
    "    saved_files = []\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file.name)\n",
    "        dest_path = os.path.join(\"./PDFs\", filename)\n",
    "\n",
    "        if os.path.abspath(file.name) != os.path.abspath(dest_path):\n",
    "            shutil.copy(file.name, dest_path)\n",
    "            saved_files.append(filename)\n",
    "        else:\n",
    "            saved_files.append(filename + \" (already exists)\")\n",
    "\n",
    "    global uploaded_pdfs\n",
    "    uploaded_pdfs = saved_files\n",
    "    return f\"Uploaded: {', '.join(saved_files)}\"\n",
    "\n",
    "# Mongo Upload Logic\n",
    "def upload_to_mongo():\n",
    "    if not uploaded_pdfs:\n",
    "        return \"‚ùå No PDFs uploaded yet.\"\n",
    "\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    collection = get_collection(client, \"pdf_rag_db\", \"pages\")\n",
    "\n",
    "    inserted = []\n",
    "    skipped = []\n",
    "\n",
    "    for pdf in uploaded_pdfs:\n",
    "        full_path = os.path.join(\"./PDFs\", pdf)\n",
    "\n",
    "        if collection.find_one({\"pdf_name\": pdf}):\n",
    "            skipped.append(pdf)\n",
    "            continue\n",
    "\n",
    "        pages = extract_text_from_pdf(full_path)\n",
    "        insert_pdf_pages_to_mongo(collection, pdf, pages)\n",
    "        inserted.append(pdf)\n",
    "\n",
    "    result = \"\"\n",
    "    if inserted:\n",
    "        result += f\"‚úÖ Uploaded to MongoDB: {', '.join(inserted)}\\n\"\n",
    "    if skipped:\n",
    "        result += f\"‚ö†Ô∏è Skipped (already in DB): {', '.join(skipped)}\"\n",
    "    return result.strip()\n",
    "\n",
    "# Add this helper function to get list of uploaded PDFs\n",
    "def get_uploaded_pdfs():\n",
    "    return [f for f in os.listdir(\"./PDFs\") if f.endswith(\".pdf\")]\n",
    "\n",
    "# ------------------ CHUNKING LOGIC ------------------\n",
    "def chunking(\n",
    "    selected_pdf,\n",
    "    db_name: str = \"pdf_rag_db\",\n",
    "    collection_name: str = \"pages\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 100\n",
    ") -> str:\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    pages_collection = get_collection(client, db_name, collection_name)\n",
    "    documents = list(pages_collection.find({\"pdf_name\": selected_pdf}))\n",
    "    if not documents:\n",
    "        return f\"‚ùå No pages found in MongoDB for {selected_pdf}.\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc[\"text\"])\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                \"pdf_name\": doc[\"pdf_name\"],\n",
    "                \"page_number\": doc[\"page_number\"],\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "    if not chunked_docs:\n",
    "        return \"‚ùå No chunks created.\"\n",
    "\n",
    "    chunk_collection = get_collection(client, db_name, \"chunks\")\n",
    "    chunk_collection.insert_many(chunked_docs)\n",
    "    return f\"‚úÖ Stored {len(chunked_docs)} chunks for {selected_pdf} in MongoDB.\"\n",
    "\n",
    "def display_chunks(selected_pdf):\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    collection = get_collection(client, \"pdf_rag_db\", \"chunks\")\n",
    "    chunks = list(collection.find({\"pdf_name\": selected_pdf}, {\"_id\": 0}))\n",
    "    return chunks if chunks else f\"‚ö†Ô∏è No chunks found in MongoDB for {selected_pdf}.\"\n",
    "\n",
    "# ------------------ EMBEDDING & CHROMADB ------------------\n",
    "def get_all_chroma_ids(collection):\n",
    "    all_ids = set()\n",
    "    offset = 0\n",
    "    batch_size = 100  # or 500, adjust as needed\n",
    "\n",
    "    while True:\n",
    "        result = collection.get(ids=None, limit=batch_size, offset=offset)\n",
    "        ids = result.get(\"ids\", [])\n",
    "        if not ids:\n",
    "            break\n",
    "        all_ids.update(ids)\n",
    "        offset += batch_size\n",
    "\n",
    "    return all_ids\n",
    "\n",
    "def embed_chunks_to_mongo(selected_pdf):\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    chunk_collection = get_collection(client, \"pdf_rag_db\", \"chunks\")\n",
    "\n",
    "    chunks = list(chunk_collection.find({\"pdf_name\": selected_pdf}))\n",
    "    if not chunks:\n",
    "        return f\"‚ùå No chunks found for {selected_pdf}.\"\n",
    "\n",
    "    # Add embedding to each chunk\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    updates = []\n",
    "    for chunk in chunks:\n",
    "        if \"embedding\" in chunk:\n",
    "            continue  # Already embedded\n",
    "\n",
    "        embedding = model.encode(chunk[\"chunk_text\"]).tolist()\n",
    "        updates.append({\n",
    "            \"filter\": {\"_id\": chunk[\"_id\"]},\n",
    "            \"update\": {\"$set\": {\"embedding\": embedding}}\n",
    "        })\n",
    "\n",
    "    for update in updates:\n",
    "        chunk_collection.update_one(update[\"filter\"], update[\"update\"])\n",
    "\n",
    "    return f\"‚úÖ Embedded and stored {len(updates)} new chunks for {selected_pdf} in MongoDB.\"\n",
    "\n",
    "\n",
    "# Load the same model used for embedding\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def search_query(question, top_k=5):\n",
    "    client = get_mongo_client(MONGO_URI)\n",
    "    chunk_collection = get_collection(client, \"pdf_rag_db\", \"chunks\")\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([question])[0]\n",
    "\n",
    "    # Find all chunks with embeddings\n",
    "    chunks = list(chunk_collection.find({\"embedding\": {\"$exists\": True}}))\n",
    "\n",
    "    # Compute similarity\n",
    "    scored = []\n",
    "    for chunk in chunks:\n",
    "        score = cosine_similarity(query_embedding, chunk[\"embedding\"])\n",
    "        scored.append((score, chunk))\n",
    "\n",
    "    # Sort by similarity\n",
    "    top_matches = sorted(scored, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "    if not top_matches:\n",
    "        return \"‚ö†Ô∏è No matching chunks found.\"\n",
    "\n",
    "    formatted = []\n",
    "    for i, (score, chunk) in enumerate(top_matches):\n",
    "        formatted.append(\n",
    "            f\"üß© Chunk {i+1} (Score: {score:.4f}) ‚Äî Page {chunk.get('page_number')}:\\n{chunk.get('chunk_text')}\"\n",
    "        )\n",
    "    return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "# ------------------ UI ------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üìÑ PDF Upload + MongoDB + Knowledge Nuggets\")\n",
    "\n",
    "    with gr.Tab(\"Upload PDFs\"):\n",
    "        upload_input = gr.File(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "        upload_result = gr.Textbox(label=\"Upload Result\")\n",
    "        mongo_result = gr.Textbox(label=\"MongoDB Result\")\n",
    "\n",
    "        upload_btn = gr.Button(\"Upload to ./PDFs\")\n",
    "        mongo_btn = gr.Button(\"Upload Extracted Text to MongoDB\")\n",
    "\n",
    "        upload_btn.click(upload_docs, inputs=upload_input, outputs=upload_result)\n",
    "        mongo_btn.click(upload_to_mongo, outputs=mongo_result)\n",
    "\n",
    "    with gr.Tab(\"Knowledge Nuggets\"):\n",
    "        pdf_select_chunk = gr.Dropdown(label=\"Select PDF to Chunk\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        chunk_btn = gr.Button(\"üî™ Chunk PDF Pages\")\n",
    "        chunk_output = gr.Textbox(label=\"Chunking Result\")\n",
    "\n",
    "        pdf_select_display = gr.Dropdown(label=\"Select PDF to Display Chunks\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        display_btn = gr.Button(\"üìö Display Chunks\")\n",
    "        display_output = gr.JSON(label=\"Chunks from MongoDB\")\n",
    "\n",
    "        chunk_btn.click(chunking, inputs=pdf_select_chunk, outputs=chunk_output)\n",
    "        display_btn.click(display_chunks, inputs=pdf_select_display, outputs=display_output)\n",
    "\n",
    "    def view_embedded_chunks(pdf_name):\n",
    "        \"\"\"\n",
    "        View all chunks for a given PDF that have been embedded (i.e., have an 'embedding' field).\n",
    "        \"\"\"\n",
    "        chunks_collection = db[\"chunks\"]\n",
    "        embedded_chunks = list(chunks_collection.find(\n",
    "            {\"pdf_name\": pdf_name, \"embedding\": {\"$exists\": True}},\n",
    "            {\"_id\": 0, \"page_number\": 1, \"chunk_index\": 1, \"chunk_text\": 1}\n",
    "        ).sort([(\"page_number\", 1), (\"chunk_index\", 1)]))\n",
    "    \n",
    "        return embedded_chunks\n",
    "        \n",
    "    with gr.Tab(\"Embed to Mongo\"):\n",
    "        # PDF selection for embedding\n",
    "        pdf_select_embed = gr.Dropdown(label=\"Select PDF to Embed\", choices=get_uploaded_pdfs(), interactive=True)\n",
    "        embed_btn = gr.Button(\"üíæ Embed and Store in ChromaDB\")\n",
    "        embed_output = gr.Textbox(label=\"Embedding Result\")\n",
    "    \n",
    "        # Embed selected PDF into ChromaDB\n",
    "        embed_btn.click(embed_chunks_to_mongo, inputs=pdf_select_embed, outputs=embed_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    with gr.Tab(\"Query Knowledge Nuggets\"):\n",
    "        gr.Markdown(\"### Ask a question about the document\")\n",
    "    \n",
    "        query_input = gr.Textbox(label=\"Enter your question\", placeholder=\"e.g. What is the document about?\")\n",
    "        query_button = gr.Button(\"Search\")\n",
    "        query_output = gr.Textbox(label=\"Top 5 Matching Chunks\", lines=15)\n",
    "    \n",
    "        query_button.click(fn=search_query, inputs=query_input, outputs=query_output)\n",
    "\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252742a-1f30-4310-a214-98565a82807f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

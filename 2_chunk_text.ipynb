{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b5b3e6-c643-4e3a-8c44-ed314ec58edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caeaa5eb-26e3-4e6a-8e36-e67e25a03b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "%run ./utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d8b9cb-5672-48b8-9b78-5d90fe60e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = get_mongo_client(uri)\n",
    "# pages_collection = get_collection(client, \"pdf_rag_db\", \"pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944c47a8-2a5f-4b0e-b65a-b73692eb9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch all pages for all PDFs\n",
    "# documents = list(pages_collection.find({}))\n",
    "# print(f\"✅ Fetched {len(documents)} page-level docs from Mongo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d25cca5e-2e05-4a0d-9c85-6c11191e5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=100,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4f8eed9-d847-40cc-9e37-1841aa2180dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_docs = []\n",
    "\n",
    "# for doc in documents:\n",
    "#     chunks = text_splitter.split_text(doc[\"text\"])\n",
    "#     for idx, chunk in enumerate(chunks):\n",
    "#         chunked_docs.append({\n",
    "#             \"pdf_name\": doc[\"pdf_name\"],\n",
    "#             \"page_number\": doc[\"page_number\"],\n",
    "#             \"chunk_index\": idx,\n",
    "#             \"chunk_text\": chunk\n",
    "#         })\n",
    "\n",
    "# print(f\"✅ Generated {len(chunked_docs)} total chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90777197-aa74-45d8-a671-0ea46867bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(chunked_docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e155cc-ad73-4b8e-8034-6c930df5bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(\n",
    "    db_name: str = \"pdf_rag_db\",\n",
    "    collection_name: str = \"pages\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 100\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch all PDF pages from MongoDB and return chunked versions of their text.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts like:\n",
    "        {\n",
    "            \"pdf_name\": str,\n",
    "            \"page_number\": int,\n",
    "            \"chunk_index\": int,\n",
    "            \"chunk_text\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB and get the pages collection\n",
    "    client = get_mongo_client(uri)\n",
    "    pages_collection = get_collection(client, db_name, collection_name)\n",
    "    \n",
    "    # Fetch all page documents\n",
    "    documents = list(pages_collection.find({}))\n",
    "    print(f\"✅ Fetched {len(documents)} pages from MongoDB.\")\n",
    "\n",
    "    # Initialize the recursive text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    # Process and chunk each document\n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc[\"text\"])\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                \"pdf_name\": doc[\"pdf_name\"],\n",
    "                \"page_number\": doc[\"page_number\"],\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "    print(f\"✅ Created {len(chunked_docs)} total chunks.\")\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4455e0ca-95d9-4950-b011-0b33308f79e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fetched 66 pages from MongoDB.\n",
      "✅ Created 344 total chunks.\n"
     ]
    }
   ],
   "source": [
    "chunked_docs = chunking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5a87f2-c541-4557-a065-de1940e41b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pdf_name': 'metformin2.pdf',\n",
       " 'page_number': 2,\n",
       " 'chunk_index': 0,\n",
       " 'chunk_text': '2\\nPRODUCT  MONOGRAPH \\n \\n \\n \\nTEVA-METFORMIN \\n(metformin hydrochloride) \\n \\n \\n500 mg and 850 mg Tablets \\n \\n \\n \\nTHERAPEUTIC  CLASSIFICATION \\n \\nOral Antihyperglycemic Agent \\n \\n \\nACTIONS  AND  CLINICAL  PHARMACOLOGY \\n \\nTEVA-METFORMIN (metformin hydrochloride) is a biguanide derivative producing an \\nantihyperglycemic effect which can only be observed in man or in the diabetic animal and only \\nwhen there is insulin secretion. Metformin, at therapeutic doses, does not cause hypoglycemia'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8246178-b64a-45ec-aa67-81664d463080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 344 chunks into MongoDB collection 'chunks'.\n"
     ]
    }
   ],
   "source": [
    "insert_chunks_to_mongo(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17a4e4-b494-4858-ad0a-87d680aebe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
